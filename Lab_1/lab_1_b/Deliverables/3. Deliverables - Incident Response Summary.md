
### **Part 3: Incident Response

**Incident Summary**

- **What failed?** The connectivity between the Application Layer (EC2) and the Data Layer (RDS) was severed due to a missing Inbound Security Group rule on TCP Port 3306.
    
- **How was it detected?** The `DBConnectionErrors` metric exceeded the threshold of 3, triggering the CloudWatch Alarm `connection-failure`, which notified the operations team via SNS (Email).
    
- **Root Cause:** Security Group configuration drift (Human Error).
    
- **Time to Recovery (MTTR):** ~ 13 Minutes
    
	- **Incident Start (Detection):** `2026-01-18 01:44:16.215Z`
		
	- **Detection:** Email receipt time: Sunday 18 January, 2026 `01:45:47 UTC`, which was Saturday 17, 2026 `8:45â€¯PM EST`
	    
	- **Last Known Failure:** `2026-01-18 01:46:16.648Z`
	    
	- **Diagnosis & Remediation:**  `2026-01-18 01:46:16.648Z` to `2026-01-18 01:57:51.446Z`
		
	- **Recovery Confirmation:** `2026-01-18 01:57:51.446Z` (Confirmed `/list` endpoint accessibility and Alarm transition to `OK`).

**Preventive Action**

1. **Reduce MTTR:** Implement a "Known Good" Security Group that can be hot-swapped onto the RDS instance during emergencies, rather than manually editing rules.
    
2. **Prevent Recurrence:** Implement **Infrastructure as Code (Terraform/CloudFormation)** and **Standard Operating Procedures (SOPs)** for managing security groups and core infrastructure. This transition will eliminate manual configuration errors, prevent drift, and enable automated **'Configuration Drift Detection'** to alert the operations team _before_ a service failure occurs.
    

---

